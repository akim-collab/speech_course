init_weights: null

model:
  model_class: 'ctc_model'
  labels: [ " ", "а", "б", "в", "г", "д", "е", "ж", "з", "и", "й", "к", "л", "м", "н", "о", "п", "р", "с", "т", "у", "ф", "х", "ц", "ч", "ш", "щ", "ъ", "ы", "ь", "э", "ю", "я" ]

  encoder:
    _target_: src.encoders.e_branchformer.EBranchformerEncoder
    subsampling_stride: 4
    features_num: 64
    d_model: &d_model 256
    layers_num: 8
    dropout: &dropout 0.1
    attn_config:
      embed_dim: *d_model
      num_heads: 4
    cgmlp_config:
      size: *d_model
      expansion_factor: 3
      kernel_size: 7
      dropout: *dropout
      use_linear_after_conv: False
    ffn_expansion_factor: 2
    merge_conv_kernel: 31

  decoder:
    _target_: src.decoders.ctc.ConvDecoder
    feat_in: ${model.encoder.d_model}
    labels: ${model.labels}

optim:
  optimizer:
    name: 
      Adam
    params:
      lr: 1e-3
  scheduler:
    name:
      CosineAnnealing
    params:
      warmup_steps: 1000
      max_steps: ${trainer.max_steps}


train_dataloader:
  num_workers: 1
  batch_size: 64
  prefetch_factor: 1
  dataset:
    manifest_name: train_opus/manifest.jsonl
    max_duration: 16.7
    min_duration: 1.0
    max_len: 68
    labels: ${model.labels}
    transforms:
      - name: mel_spectrogram
        params:
          sample_rate: 16000
          n_fft: 400
          win_length: 400
          hop_length: 160
          n_mels: ${model.encoder.features_num}
      - name: log_scaler

val_dataloader:
  num_workers: ${train_dataloader.num_workers}
  batch_size: ${train_dataloader.batch_size}
  prefetch_factor: ${train_dataloader.prefetch_factor}
  dataset:
    manifest_name: test_opus/crowd/manifest.jsonl
    labels: ${model.labels}
    transforms:
      ${train_dataloader.dataset.transforms}

trainer:
  check_val_every_n_epoch: 2
  log_every_n_steps: 1000
  precision: 32
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  accelerator: auto
  max_steps: 30000
  devices: 1